{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**TODO : LOADING OUR DATA**"],"metadata":{"id":"CyKD78DnKbLP"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vcouJCjxMMjo","executionInfo":{"status":"ok","timestamp":1728578064458,"user_tz":300,"elapsed":4623,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"515f0c1c-474a-452a-c815-e5debcf5681e"},"outputs":[{"output_type":"stream","name":"stdout","text":["> Input shape: (353, 10) for training, (89, 10) for testing\n","> Label shape: (353, 1) for training, (89, 1) for testing\n"]}],"source":["import numpy as np\n","from sklearn.datasets import load_diabetes\n","from sklearn.model_selection import train_test_split\n","\n","# Load diabetes dataset\n","diabetes = load_diabetes()\n","X, Y = diabetes.data, diabetes.target\n","\n","# TODO: Split the data into a 80%-20% training-testing split\n","X0, X1, Y0, Y1 = train_test_split(X, Y, test_size=0.2, random_state=42)\n","\n","# TODO: Reshape the Y subsets to have shape (num_samples, 1)\n","Y0 = Y0.reshape(-1, 1)\n","Y1 = Y1.reshape(-1, 1)\n","\n","# Print out the shapes\n","print(f\"\"\"\n","> Input shape: {X0.shape} for training, {X1.shape} for testing\n","> Label shape: {Y0.shape} for training, {Y1.shape} for testing\n","\"\"\".strip())"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class Regression(nn.Module):\n","\n","    \"\"\"\n","    Initialize all the inherent \"things\" inside of a model!\n","    This includes things like the layers, activation/loss functions, and optimzer.\n","    \"\"\"\n","    def __init__(self, input_dims, output_dims):\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        super().__init__()\n","        self.dense = nn.Linear(input_dims, output_dims).to(self.device)\n","        self.activation = None  ## To be specified in subclasses\n","        self.loss = None        ## To be specified in subclasses\n","        self.set_learning_rate()\n","\n","    \"\"\"\n","    Sets up the optmizer\n","    \"\"\"\n","    def set_learning_rate(self, learning_rate=0.001):\n","        self.optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate) ## Simple stochastic gradient descent (SGD) optimizer\n","\n","    \"\"\"\n","    Forward pass of the model\n","    Given an input x, how does the model process the input to get its output?\n","    \"\"\"\n","    def forward(self, x):\n","        x = self.dense(x)\n","        x = self.activation(x)\n","        return x"],"metadata":{"id":"vbhTnnnPOKuS","executionInfo":{"status":"ok","timestamp":1728578077547,"user_tz":300,"elapsed":10679,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class TrainTest:\n","\n","    no_grad = torch.no_grad\n","\n","    def fit(self, data):\n","        ## Training loop\n","        self.train()        ## Set model into training mode\n","        ## Iterate over the data batches\n","        for batch, (inputs, target) in enumerate(data):\n","            ## In real pytorch, you'd need to set the device\n","            inputs = inputs.to(self.device)\n","            target = target.to(self.device)\n","            ## Erase the gradient history\n","            self.optimizer.zero_grad()\n","            ## Do a forward pass on the model\n","            output = self(inputs)\n","            ## Compute the loss\n","            loss = self.loss(output, target)\n","            ## Run backwards pass from the loss through the previous layers\n","            ## This will accumulate gradients for the parameters that need to be optimized\n","            loss.backward()\n","            ## Perform a single optimization step\n","            self.optimizer.step()\n","        return {'loss' : loss}\n","\n","    def evaluate(self, data):\n","        ## Set model into \"evaluate\" mode so that the parameters don't get updated\n","        self.eval()\n","        total_loss = 0\n","        ## Cut off the tensor training scope to make sure weights aren't updated\n","        ## For now, it's torch.no_grad; later, you'll use Tensor.no_grad\n","        with TrainTest.no_grad():\n","            for inputs, target in data:\n","                ## In real pytorch, you'd need to set the device\n","                inputs = inputs.to(self.device)\n","                target = target.to(self.device)\n","                output = self(inputs)\n","                total_loss += self.loss(output, target).item()  # sum up batch loss\n","\n","        total_loss /= len(data)\n","        return {'test_loss' : total_loss}\n","\n","    def train_test(self, train_data, test_data, epochs=1):\n","        ## Does both training and validation on a per-epoch basis\n","        all_stats = []\n","        for epoch in range(epochs):\n","            train_stats = self.fit(train_data)\n","            test_stats = self.evaluate(test_data)\n","            all_stats += [{**train_stats, **test_stats}]\n","            print(f'[Epoch {epoch+1}/{epochs}]', all_stats[-1])\n","        return all_stats"],"metadata":{"id":"0RNbiGpuOPTn","executionInfo":{"status":"ok","timestamp":1728578077548,"user_tz":300,"elapsed":3,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class LinearRegression(Regression, TrainTest):\n","    def __init__(self, input_dims, output_dims):\n","        super().__init__(input_dims, output_dims)\n","        self.activation = nn.Identity()\n","        self.loss = nn.MSELoss()"],"metadata":{"id":"7cvwgcrhOXy1","executionInfo":{"status":"ok","timestamp":1728578077548,"user_tz":300,"elapsed":2,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["torch_model = LinearRegression(X0.shape[-1], 1)\n","torch_model.set_learning_rate(0.3)\n","torch_model_stats = torch_model.train_test(\n","    [[torch.Tensor(X0), torch.Tensor(Y0)]],\n","    [[torch.Tensor(X1), torch.Tensor(Y1)]],\n","    epochs=200\n",");"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JxS49BzKz_f","executionInfo":{"status":"ok","timestamp":1728578085494,"user_tz":300,"elapsed":7814,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"ce48c9f7-30a1-4979-d1c2-3d761d97dabb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[Epoch 1/200] {'loss': tensor(29722.4570, grad_fn=<MseLossBackward0>), 'test_loss': 8145.8701171875}\n","[Epoch 2/200] {'loss': tensor(9832.9111, grad_fn=<MseLossBackward0>), 'test_loss': 5531.677734375}\n","[Epoch 3/200] {'loss': tensor(6630.8345, grad_fn=<MseLossBackward0>), 'test_loss': 5234.345703125}\n","[Epoch 4/200] {'loss': tensor(6098.8306, grad_fn=<MseLossBackward0>), 'test_loss': 5223.90185546875}\n","[Epoch 5/200] {'loss': tensor(5994.2090, grad_fn=<MseLossBackward0>), 'test_loss': 5226.119140625}\n","[Epoch 6/200] {'loss': tensor(5958.1499, grad_fn=<MseLossBackward0>), 'test_loss': 5217.24169921875}\n","[Epoch 7/200] {'loss': tensor(5933.2441, grad_fn=<MseLossBackward0>), 'test_loss': 5201.47705078125}\n","[Epoch 8/200] {'loss': tensor(5910.3037, grad_fn=<MseLossBackward0>), 'test_loss': 5182.68310546875}\n","[Epoch 9/200] {'loss': tensor(5887.8564, grad_fn=<MseLossBackward0>), 'test_loss': 5162.744140625}\n","[Epoch 10/200] {'loss': tensor(5865.6650, grad_fn=<MseLossBackward0>), 'test_loss': 5142.46435546875}\n","[Epoch 11/200] {'loss': tensor(5843.6909, grad_fn=<MseLossBackward0>), 'test_loss': 5122.171875}\n","[Epoch 12/200] {'loss': tensor(5821.9248, grad_fn=<MseLossBackward0>), 'test_loss': 5101.9970703125}\n","[Epoch 13/200] {'loss': tensor(5800.3638, grad_fn=<MseLossBackward0>), 'test_loss': 5081.9912109375}\n","[Epoch 14/200] {'loss': tensor(5779.0049, grad_fn=<MseLossBackward0>), 'test_loss': 5062.17333984375}\n","[Epoch 15/200] {'loss': tensor(5757.8477, grad_fn=<MseLossBackward0>), 'test_loss': 5042.54931640625}\n","[Epoch 16/200] {'loss': tensor(5736.8896, grad_fn=<MseLossBackward0>), 'test_loss': 5023.119140625}\n","[Epoch 17/200] {'loss': tensor(5716.1274, grad_fn=<MseLossBackward0>), 'test_loss': 5003.88427734375}\n","[Epoch 18/200] {'loss': tensor(5695.5601, grad_fn=<MseLossBackward0>), 'test_loss': 4984.8427734375}\n","[Epoch 19/200] {'loss': tensor(5675.1855, grad_fn=<MseLossBackward0>), 'test_loss': 4965.99169921875}\n","[Epoch 20/200] {'loss': tensor(5655.0015, grad_fn=<MseLossBackward0>), 'test_loss': 4947.33056640625}\n","[Epoch 21/200] {'loss': tensor(5635.0063, grad_fn=<MseLossBackward0>), 'test_loss': 4928.8564453125}\n","[Epoch 22/200] {'loss': tensor(5615.1978, grad_fn=<MseLossBackward0>), 'test_loss': 4910.56689453125}\n","[Epoch 23/200] {'loss': tensor(5595.5742, grad_fn=<MseLossBackward0>), 'test_loss': 4892.46044921875}\n","[Epoch 24/200] {'loss': tensor(5576.1328, grad_fn=<MseLossBackward0>), 'test_loss': 4874.5361328125}\n","[Epoch 25/200] {'loss': tensor(5556.8730, grad_fn=<MseLossBackward0>), 'test_loss': 4856.79052734375}\n","[Epoch 26/200] {'loss': tensor(5537.7920, grad_fn=<MseLossBackward0>), 'test_loss': 4839.22265625}\n","[Epoch 27/200] {'loss': tensor(5518.8887, grad_fn=<MseLossBackward0>), 'test_loss': 4821.83056640625}\n","[Epoch 28/200] {'loss': tensor(5500.1611, grad_fn=<MseLossBackward0>), 'test_loss': 4804.61181640625}\n","[Epoch 29/200] {'loss': tensor(5481.6069, grad_fn=<MseLossBackward0>), 'test_loss': 4787.5654296875}\n","[Epoch 30/200] {'loss': tensor(5463.2241, grad_fn=<MseLossBackward0>), 'test_loss': 4770.689453125}\n","[Epoch 31/200] {'loss': tensor(5445.0122, grad_fn=<MseLossBackward0>), 'test_loss': 4753.98095703125}\n","[Epoch 32/200] {'loss': tensor(5426.9683, grad_fn=<MseLossBackward0>), 'test_loss': 4737.43994140625}\n","[Epoch 33/200] {'loss': tensor(5409.0908, grad_fn=<MseLossBackward0>), 'test_loss': 4721.06298828125}\n","[Epoch 34/200] {'loss': tensor(5391.3794, grad_fn=<MseLossBackward0>), 'test_loss': 4704.8505859375}\n","[Epoch 35/200] {'loss': tensor(5373.8306, grad_fn=<MseLossBackward0>), 'test_loss': 4688.7978515625}\n","[Epoch 36/200] {'loss': tensor(5356.4434, grad_fn=<MseLossBackward0>), 'test_loss': 4672.9052734375}\n","[Epoch 37/200] {'loss': tensor(5339.2158, grad_fn=<MseLossBackward0>), 'test_loss': 4657.1708984375}\n","[Epoch 38/200] {'loss': tensor(5322.1465, grad_fn=<MseLossBackward0>), 'test_loss': 4641.5927734375}\n","[Epoch 39/200] {'loss': tensor(5305.2349, grad_fn=<MseLossBackward0>), 'test_loss': 4626.16943359375}\n","[Epoch 40/200] {'loss': tensor(5288.4766, grad_fn=<MseLossBackward0>), 'test_loss': 4610.89892578125}\n","[Epoch 41/200] {'loss': tensor(5271.8726, grad_fn=<MseLossBackward0>), 'test_loss': 4595.77978515625}\n","[Epoch 42/200] {'loss': tensor(5255.4209, grad_fn=<MseLossBackward0>), 'test_loss': 4580.810546875}\n","[Epoch 43/200] {'loss': tensor(5239.1191, grad_fn=<MseLossBackward0>), 'test_loss': 4565.9892578125}\n","[Epoch 44/200] {'loss': tensor(5222.9663, grad_fn=<MseLossBackward0>), 'test_loss': 4551.3154296875}\n","[Epoch 45/200] {'loss': tensor(5206.9609, grad_fn=<MseLossBackward0>), 'test_loss': 4536.78662109375}\n","[Epoch 46/200] {'loss': tensor(5191.1016, grad_fn=<MseLossBackward0>), 'test_loss': 4522.40087890625}\n","[Epoch 47/200] {'loss': tensor(5175.3857, grad_fn=<MseLossBackward0>), 'test_loss': 4508.15771484375}\n","[Epoch 48/200] {'loss': tensor(5159.8130, grad_fn=<MseLossBackward0>), 'test_loss': 4494.05517578125}\n","[Epoch 49/200] {'loss': tensor(5144.3823, grad_fn=<MseLossBackward0>), 'test_loss': 4480.091796875}\n","[Epoch 50/200] {'loss': tensor(5129.0908, grad_fn=<MseLossBackward0>), 'test_loss': 4466.265625}\n","[Epoch 51/200] {'loss': tensor(5113.9385, grad_fn=<MseLossBackward0>), 'test_loss': 4452.57666015625}\n","[Epoch 52/200] {'loss': tensor(5098.9224, grad_fn=<MseLossBackward0>), 'test_loss': 4439.021484375}\n","[Epoch 53/200] {'loss': tensor(5084.0425, grad_fn=<MseLossBackward0>), 'test_loss': 4425.6005859375}\n","[Epoch 54/200] {'loss': tensor(5069.2969, grad_fn=<MseLossBackward0>), 'test_loss': 4412.31103515625}\n","[Epoch 55/200] {'loss': tensor(5054.6846, grad_fn=<MseLossBackward0>), 'test_loss': 4399.15234375}\n","[Epoch 56/200] {'loss': tensor(5040.2026, grad_fn=<MseLossBackward0>), 'test_loss': 4386.12255859375}\n","[Epoch 57/200] {'loss': tensor(5025.8525, grad_fn=<MseLossBackward0>), 'test_loss': 4373.22119140625}\n","[Epoch 58/200] {'loss': tensor(5011.6299, grad_fn=<MseLossBackward0>), 'test_loss': 4360.44677734375}\n","[Epoch 59/200] {'loss': tensor(4997.5361, grad_fn=<MseLossBackward0>), 'test_loss': 4347.79638671875}\n","[Epoch 60/200] {'loss': tensor(4983.5684, grad_fn=<MseLossBackward0>), 'test_loss': 4335.2705078125}\n","[Epoch 61/200] {'loss': tensor(4969.7251, grad_fn=<MseLossBackward0>), 'test_loss': 4322.8671875}\n","[Epoch 62/200] {'loss': tensor(4956.0059, grad_fn=<MseLossBackward0>), 'test_loss': 4310.58544921875}\n","[Epoch 63/200] {'loss': tensor(4942.4092, grad_fn=<MseLossBackward0>), 'test_loss': 4298.42333984375}\n","[Epoch 64/200] {'loss': tensor(4928.9341, grad_fn=<MseLossBackward0>), 'test_loss': 4286.38037109375}\n","[Epoch 65/200] {'loss': tensor(4915.5786, grad_fn=<MseLossBackward0>), 'test_loss': 4274.455078125}\n","[Epoch 66/200] {'loss': tensor(4902.3423, grad_fn=<MseLossBackward0>), 'test_loss': 4262.6455078125}\n","[Epoch 67/200] {'loss': tensor(4889.2231, grad_fn=<MseLossBackward0>), 'test_loss': 4250.951171875}\n","[Epoch 68/200] {'loss': tensor(4876.2202, grad_fn=<MseLossBackward0>), 'test_loss': 4239.37158203125}\n","[Epoch 69/200] {'loss': tensor(4863.3335, grad_fn=<MseLossBackward0>), 'test_loss': 4227.90380859375}\n","[Epoch 70/200] {'loss': tensor(4850.5601, grad_fn=<MseLossBackward0>), 'test_loss': 4216.54833984375}\n","[Epoch 71/200] {'loss': tensor(4837.9004, grad_fn=<MseLossBackward0>), 'test_loss': 4205.30224609375}\n","[Epoch 72/200] {'loss': tensor(4825.3521, grad_fn=<MseLossBackward0>), 'test_loss': 4194.166015625}\n","[Epoch 73/200] {'loss': tensor(4812.9141, grad_fn=<MseLossBackward0>), 'test_loss': 4183.13818359375}\n","[Epoch 74/200] {'loss': tensor(4800.5859, grad_fn=<MseLossBackward0>), 'test_loss': 4172.216796875}\n","[Epoch 75/200] {'loss': tensor(4788.3667, grad_fn=<MseLossBackward0>), 'test_loss': 4161.40087890625}\n","[Epoch 76/200] {'loss': tensor(4776.2544, grad_fn=<MseLossBackward0>), 'test_loss': 4150.6904296875}\n","[Epoch 77/200] {'loss': tensor(4764.2480, grad_fn=<MseLossBackward0>), 'test_loss': 4140.0830078125}\n","[Epoch 78/200] {'loss': tensor(4752.3472, grad_fn=<MseLossBackward0>), 'test_loss': 4129.57861328125}\n","[Epoch 79/200] {'loss': tensor(4740.5508, grad_fn=<MseLossBackward0>), 'test_loss': 4119.17626953125}\n","[Epoch 80/200] {'loss': tensor(4728.8569, grad_fn=<MseLossBackward0>), 'test_loss': 4108.873046875}\n","[Epoch 81/200] {'loss': tensor(4717.2661, grad_fn=<MseLossBackward0>), 'test_loss': 4098.669921875}\n","[Epoch 82/200] {'loss': tensor(4705.7749, grad_fn=<MseLossBackward0>), 'test_loss': 4088.56494140625}\n","[Epoch 83/200] {'loss': tensor(4694.3848, grad_fn=<MseLossBackward0>), 'test_loss': 4078.5576171875}\n","[Epoch 84/200] {'loss': tensor(4683.0933, grad_fn=<MseLossBackward0>), 'test_loss': 4068.64599609375}\n","[Epoch 85/200] {'loss': tensor(4671.8994, grad_fn=<MseLossBackward0>), 'test_loss': 4058.830322265625}\n","[Epoch 86/200] {'loss': tensor(4660.8032, grad_fn=<MseLossBackward0>), 'test_loss': 4049.108154296875}\n","[Epoch 87/200] {'loss': tensor(4649.8032, grad_fn=<MseLossBackward0>), 'test_loss': 4039.480224609375}\n","[Epoch 88/200] {'loss': tensor(4638.8979, grad_fn=<MseLossBackward0>), 'test_loss': 4029.944091796875}\n","[Epoch 89/200] {'loss': tensor(4628.0869, grad_fn=<MseLossBackward0>), 'test_loss': 4020.499755859375}\n","[Epoch 90/200] {'loss': tensor(4617.3687, grad_fn=<MseLossBackward0>), 'test_loss': 4011.14599609375}\n","[Epoch 91/200] {'loss': tensor(4606.7427, grad_fn=<MseLossBackward0>), 'test_loss': 4001.88134765625}\n","[Epoch 92/200] {'loss': tensor(4596.2090, grad_fn=<MseLossBackward0>), 'test_loss': 3992.705078125}\n","[Epoch 93/200] {'loss': tensor(4585.7646, grad_fn=<MseLossBackward0>), 'test_loss': 3983.616943359375}\n","[Epoch 94/200] {'loss': tensor(4575.4106, grad_fn=<MseLossBackward0>), 'test_loss': 3974.615478515625}\n","[Epoch 95/200] {'loss': tensor(4565.1450, grad_fn=<MseLossBackward0>), 'test_loss': 3965.699462890625}\n","[Epoch 96/200] {'loss': tensor(4554.9673, grad_fn=<MseLossBackward0>), 'test_loss': 3956.86865234375}\n","[Epoch 97/200] {'loss': tensor(4544.8760, grad_fn=<MseLossBackward0>), 'test_loss': 3948.12255859375}\n","[Epoch 98/200] {'loss': tensor(4534.8711, grad_fn=<MseLossBackward0>), 'test_loss': 3939.458984375}\n","[Epoch 99/200] {'loss': tensor(4524.9507, grad_fn=<MseLossBackward0>), 'test_loss': 3930.87744140625}\n","[Epoch 100/200] {'loss': tensor(4515.1152, grad_fn=<MseLossBackward0>), 'test_loss': 3922.378173828125}\n","[Epoch 101/200] {'loss': tensor(4505.3633, grad_fn=<MseLossBackward0>), 'test_loss': 3913.959228515625}\n","[Epoch 102/200] {'loss': tensor(4495.6938, grad_fn=<MseLossBackward0>), 'test_loss': 3905.6201171875}\n","[Epoch 103/200] {'loss': tensor(4486.1064, grad_fn=<MseLossBackward0>), 'test_loss': 3897.359619140625}\n","[Epoch 104/200] {'loss': tensor(4476.5991, grad_fn=<MseLossBackward0>), 'test_loss': 3889.177734375}\n","[Epoch 105/200] {'loss': tensor(4467.1729, grad_fn=<MseLossBackward0>), 'test_loss': 3881.07275390625}\n","[Epoch 106/200] {'loss': tensor(4457.8252, grad_fn=<MseLossBackward0>), 'test_loss': 3873.044921875}\n","[Epoch 107/200] {'loss': tensor(4448.5571, grad_fn=<MseLossBackward0>), 'test_loss': 3865.0927734375}\n","[Epoch 108/200] {'loss': tensor(4439.3667, grad_fn=<MseLossBackward0>), 'test_loss': 3857.21533203125}\n","[Epoch 109/200] {'loss': tensor(4430.2534, grad_fn=<MseLossBackward0>), 'test_loss': 3849.412109375}\n","[Epoch 110/200] {'loss': tensor(4421.2158, grad_fn=<MseLossBackward0>), 'test_loss': 3841.6826171875}\n","[Epoch 111/200] {'loss': tensor(4412.2539, grad_fn=<MseLossBackward0>), 'test_loss': 3834.025634765625}\n","[Epoch 112/200] {'loss': tensor(4403.3672, grad_fn=<MseLossBackward0>), 'test_loss': 3826.4404296875}\n","[Epoch 113/200] {'loss': tensor(4394.5547, grad_fn=<MseLossBackward0>), 'test_loss': 3818.92626953125}\n","[Epoch 114/200] {'loss': tensor(4385.8149, grad_fn=<MseLossBackward0>), 'test_loss': 3811.483154296875}\n","[Epoch 115/200] {'loss': tensor(4377.1479, grad_fn=<MseLossBackward0>), 'test_loss': 3804.109619140625}\n","[Epoch 116/200] {'loss': tensor(4368.5522, grad_fn=<MseLossBackward0>), 'test_loss': 3796.804443359375}\n","[Epoch 117/200] {'loss': tensor(4360.0288, grad_fn=<MseLossBackward0>), 'test_loss': 3789.568115234375}\n","[Epoch 118/200] {'loss': tensor(4351.5752, grad_fn=<MseLossBackward0>), 'test_loss': 3782.399658203125}\n","[Epoch 119/200] {'loss': tensor(4343.1919, grad_fn=<MseLossBackward0>), 'test_loss': 3775.298095703125}\n","[Epoch 120/200] {'loss': tensor(4334.8770, grad_fn=<MseLossBackward0>), 'test_loss': 3768.26220703125}\n","[Epoch 121/200] {'loss': tensor(4326.6313, grad_fn=<MseLossBackward0>), 'test_loss': 3761.292236328125}\n","[Epoch 122/200] {'loss': tensor(4318.4526, grad_fn=<MseLossBackward0>), 'test_loss': 3754.386962890625}\n","[Epoch 123/200] {'loss': tensor(4310.3413, grad_fn=<MseLossBackward0>), 'test_loss': 3747.5458984375}\n","[Epoch 124/200] {'loss': tensor(4302.2969, grad_fn=<MseLossBackward0>), 'test_loss': 3740.7685546875}\n","[Epoch 125/200] {'loss': tensor(4294.3179, grad_fn=<MseLossBackward0>), 'test_loss': 3734.053955078125}\n","[Epoch 126/200] {'loss': tensor(4286.4038, grad_fn=<MseLossBackward0>), 'test_loss': 3727.4013671875}\n","[Epoch 127/200] {'loss': tensor(4278.5542, grad_fn=<MseLossBackward0>), 'test_loss': 3720.810302734375}\n","[Epoch 128/200] {'loss': tensor(4270.7686, grad_fn=<MseLossBackward0>), 'test_loss': 3714.281005859375}\n","[Epoch 129/200] {'loss': tensor(4263.0459, grad_fn=<MseLossBackward0>), 'test_loss': 3707.81103515625}\n","[Epoch 130/200] {'loss': tensor(4255.3867, grad_fn=<MseLossBackward0>), 'test_loss': 3701.402099609375}\n","[Epoch 131/200] {'loss': tensor(4247.7886, grad_fn=<MseLossBackward0>), 'test_loss': 3695.051025390625}\n","[Epoch 132/200] {'loss': tensor(4240.2520, grad_fn=<MseLossBackward0>), 'test_loss': 3688.759521484375}\n","[Epoch 133/200] {'loss': tensor(4232.7764, grad_fn=<MseLossBackward0>), 'test_loss': 3682.52490234375}\n","[Epoch 134/200] {'loss': tensor(4225.3613, grad_fn=<MseLossBackward0>), 'test_loss': 3676.348388671875}\n","[Epoch 135/200] {'loss': tensor(4218.0054, grad_fn=<MseLossBackward0>), 'test_loss': 3670.227783203125}\n","[Epoch 136/200] {'loss': tensor(4210.7090, grad_fn=<MseLossBackward0>), 'test_loss': 3664.16357421875}\n","[Epoch 137/200] {'loss': tensor(4203.4707, grad_fn=<MseLossBackward0>), 'test_loss': 3658.1552734375}\n","[Epoch 138/200] {'loss': tensor(4196.2896, grad_fn=<MseLossBackward0>), 'test_loss': 3652.2021484375}\n","[Epoch 139/200] {'loss': tensor(4189.1670, grad_fn=<MseLossBackward0>), 'test_loss': 3646.303466796875}\n","[Epoch 140/200] {'loss': tensor(4182.1011, grad_fn=<MseLossBackward0>), 'test_loss': 3640.458251953125}\n","[Epoch 141/200] {'loss': tensor(4175.0908, grad_fn=<MseLossBackward0>), 'test_loss': 3634.66650390625}\n","[Epoch 142/200] {'loss': tensor(4168.1367, grad_fn=<MseLossBackward0>), 'test_loss': 3628.92724609375}\n","[Epoch 143/200] {'loss': tensor(4161.2373, grad_fn=<MseLossBackward0>), 'test_loss': 3623.240966796875}\n","[Epoch 144/200] {'loss': tensor(4154.3926, grad_fn=<MseLossBackward0>), 'test_loss': 3617.60595703125}\n","[Epoch 145/200] {'loss': tensor(4147.6021, grad_fn=<MseLossBackward0>), 'test_loss': 3612.021728515625}\n","[Epoch 146/200] {'loss': tensor(4140.8652, grad_fn=<MseLossBackward0>), 'test_loss': 3606.489013671875}\n","[Epoch 147/200] {'loss': tensor(4134.1816, grad_fn=<MseLossBackward0>), 'test_loss': 3601.005615234375}\n","[Epoch 148/200] {'loss': tensor(4127.5503, grad_fn=<MseLossBackward0>), 'test_loss': 3595.572265625}\n","[Epoch 149/200] {'loss': tensor(4120.9707, grad_fn=<MseLossBackward0>), 'test_loss': 3590.18896484375}\n","[Epoch 150/200] {'loss': tensor(4114.4429, grad_fn=<MseLossBackward0>), 'test_loss': 3584.853271484375}\n","[Epoch 151/200] {'loss': tensor(4107.9658, grad_fn=<MseLossBackward0>), 'test_loss': 3579.56591796875}\n","[Epoch 152/200] {'loss': tensor(4101.5396, grad_fn=<MseLossBackward0>), 'test_loss': 3574.326171875}\n","[Epoch 153/200] {'loss': tensor(4095.1633, grad_fn=<MseLossBackward0>), 'test_loss': 3569.133544921875}\n","[Epoch 154/200] {'loss': tensor(4088.8372, grad_fn=<MseLossBackward0>), 'test_loss': 3563.988525390625}\n","[Epoch 155/200] {'loss': tensor(4082.5591, grad_fn=<MseLossBackward0>), 'test_loss': 3558.888427734375}\n","[Epoch 156/200] {'loss': tensor(4076.3303, grad_fn=<MseLossBackward0>), 'test_loss': 3553.834716796875}\n","[Epoch 157/200] {'loss': tensor(4070.1492, grad_fn=<MseLossBackward0>), 'test_loss': 3548.826171875}\n","[Epoch 158/200] {'loss': tensor(4064.0164, grad_fn=<MseLossBackward0>), 'test_loss': 3543.8623046875}\n","[Epoch 159/200] {'loss': tensor(4057.9307, grad_fn=<MseLossBackward0>), 'test_loss': 3538.943115234375}\n","[Epoch 160/200] {'loss': tensor(4051.8914, grad_fn=<MseLossBackward0>), 'test_loss': 3534.068115234375}\n","[Epoch 161/200] {'loss': tensor(4045.8987, grad_fn=<MseLossBackward0>), 'test_loss': 3529.236328125}\n","[Epoch 162/200] {'loss': tensor(4039.9514, grad_fn=<MseLossBackward0>), 'test_loss': 3524.447265625}\n","[Epoch 163/200] {'loss': tensor(4034.0500, grad_fn=<MseLossBackward0>), 'test_loss': 3519.701171875}\n","[Epoch 164/200] {'loss': tensor(4028.1936, grad_fn=<MseLossBackward0>), 'test_loss': 3514.9970703125}\n","[Epoch 165/200] {'loss': tensor(4022.3821, grad_fn=<MseLossBackward0>), 'test_loss': 3510.3349609375}\n","[Epoch 166/200] {'loss': tensor(4016.6143, grad_fn=<MseLossBackward0>), 'test_loss': 3505.714111328125}\n","[Epoch 167/200] {'loss': tensor(4010.8909, grad_fn=<MseLossBackward0>), 'test_loss': 3501.134033203125}\n","[Epoch 168/200] {'loss': tensor(4005.2097, grad_fn=<MseLossBackward0>), 'test_loss': 3496.5947265625}\n","[Epoch 169/200] {'loss': tensor(3999.5730, grad_fn=<MseLossBackward0>), 'test_loss': 3492.09521484375}\n","[Epoch 170/200] {'loss': tensor(3993.9778, grad_fn=<MseLossBackward0>), 'test_loss': 3487.635498046875}\n","[Epoch 171/200] {'loss': tensor(3988.4246, grad_fn=<MseLossBackward0>), 'test_loss': 3483.215576171875}\n","[Epoch 172/200] {'loss': tensor(3982.9136, grad_fn=<MseLossBackward0>), 'test_loss': 3478.833984375}\n","[Epoch 173/200] {'loss': tensor(3977.4436, grad_fn=<MseLossBackward0>), 'test_loss': 3474.4912109375}\n","[Epoch 174/200] {'loss': tensor(3972.0144, grad_fn=<MseLossBackward0>), 'test_loss': 3470.1865234375}\n","[Epoch 175/200] {'loss': tensor(3966.6265, grad_fn=<MseLossBackward0>), 'test_loss': 3465.919189453125}\n","[Epoch 176/200] {'loss': tensor(3961.2781, grad_fn=<MseLossBackward0>), 'test_loss': 3461.689697265625}\n","[Epoch 177/200] {'loss': tensor(3955.9695, grad_fn=<MseLossBackward0>), 'test_loss': 3457.4970703125}\n","[Epoch 178/200] {'loss': tensor(3950.7007, grad_fn=<MseLossBackward0>), 'test_loss': 3453.3408203125}\n","[Epoch 179/200] {'loss': tensor(3945.4707, grad_fn=<MseLossBackward0>), 'test_loss': 3449.22119140625}\n","[Epoch 180/200] {'loss': tensor(3940.2791, grad_fn=<MseLossBackward0>), 'test_loss': 3445.1376953125}\n","[Epoch 181/200] {'loss': tensor(3935.1260, grad_fn=<MseLossBackward0>), 'test_loss': 3441.0888671875}\n","[Epoch 182/200] {'loss': tensor(3930.0112, grad_fn=<MseLossBackward0>), 'test_loss': 3437.075927734375}\n","[Epoch 183/200] {'loss': tensor(3924.9338, grad_fn=<MseLossBackward0>), 'test_loss': 3433.097900390625}\n","[Epoch 184/200] {'loss': tensor(3919.8938, grad_fn=<MseLossBackward0>), 'test_loss': 3429.154052734375}\n","[Epoch 185/200] {'loss': tensor(3914.8901, grad_fn=<MseLossBackward0>), 'test_loss': 3425.244140625}\n","[Epoch 186/200] {'loss': tensor(3909.9236, grad_fn=<MseLossBackward0>), 'test_loss': 3421.368408203125}\n","[Epoch 187/200] {'loss': tensor(3904.9929, grad_fn=<MseLossBackward0>), 'test_loss': 3417.525634765625}\n","[Epoch 188/200] {'loss': tensor(3900.0984, grad_fn=<MseLossBackward0>), 'test_loss': 3413.71630859375}\n","[Epoch 189/200] {'loss': tensor(3895.2393, grad_fn=<MseLossBackward0>), 'test_loss': 3409.9404296875}\n","[Epoch 190/200] {'loss': tensor(3890.4153, grad_fn=<MseLossBackward0>), 'test_loss': 3406.195556640625}\n","[Epoch 191/200] {'loss': tensor(3885.6265, grad_fn=<MseLossBackward0>), 'test_loss': 3402.48388671875}\n","[Epoch 192/200] {'loss': tensor(3880.8721, grad_fn=<MseLossBackward0>), 'test_loss': 3398.8037109375}\n","[Epoch 193/200] {'loss': tensor(3876.1519, grad_fn=<MseLossBackward0>), 'test_loss': 3395.1552734375}\n","[Epoch 194/200] {'loss': tensor(3871.4648, grad_fn=<MseLossBackward0>), 'test_loss': 3391.537841796875}\n","[Epoch 195/200] {'loss': tensor(3866.8127, grad_fn=<MseLossBackward0>), 'test_loss': 3387.95166015625}\n","[Epoch 196/200] {'loss': tensor(3862.1934, grad_fn=<MseLossBackward0>), 'test_loss': 3384.395751953125}\n","[Epoch 197/200] {'loss': tensor(3857.6069, grad_fn=<MseLossBackward0>), 'test_loss': 3380.8701171875}\n","[Epoch 198/200] {'loss': tensor(3853.0537, grad_fn=<MseLossBackward0>), 'test_loss': 3377.374267578125}\n","[Epoch 199/200] {'loss': tensor(3848.5320, grad_fn=<MseLossBackward0>), 'test_loss': 3373.907958984375}\n","[Epoch 200/200] {'loss': tensor(3844.0430, grad_fn=<MseLossBackward0>), 'test_loss': 3370.471923828125}\n"]}]},{"cell_type":"code","source":["y_true = torch.Tensor(Y0)\n","y_pred = torch_model(torch.Tensor(X0))\n","loss = torch_model.loss(y_true, y_pred)\n","\n","print(f\"\"\"\n","> Prediction Shape: {y_pred.shape}\n","> Weights    Shape: {list(torch_model.parameters())[0].shape}\n","> Bias       Shape: {list(torch_model.parameters())[1].shape}\n","> Loss       Shape: {loss.shape}\n","\"\"\".strip())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzDZc1qWLOCq","executionInfo":{"status":"ok","timestamp":1728578152167,"user_tz":300,"elapsed":141,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"b339bee5-64cb-451e-eef8-6b4c95621b84"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["> Prediction Shape: torch.Size([353, 1])\n","> Weights    Shape: torch.Size([1, 10])\n","> Bias       Shape: torch.Size([1])\n","> Loss       Shape: torch.Size([])\n"]}]},{"cell_type":"code","source":["class Tensor(np.ndarray):\n","\n","    '''\n","    Subclassing numpy arrays is a bit weird:\n","    https://numpy.org/doc/stable/user/basics.subclassing.html\n","\n","    Just assume that the attributes referred to in __new__/__array_finalize__\n","    will be accessible in a Tensor when a new Tensor object is created.\n","    '''\n","\n","    requires_grad = True  ## Class variable; accessible by Tensor.requires_grad\n","\n","    def __new__(cls, input_array):\n","        obj = np.asarray(input_array).view(cls)\n","        obj.backward = lambda x: None   ## Backward starts as None, gets assigned later\n","        obj.grad = None                 ## Gradient starts as None, gets computed later\n","        obj.requires_grad = True        ## By default, we'll want to compute gradient for new tensors\n","        obj.to = lambda x: obj          ## We don't handle special device support (i.e. cpu vs gpu/cuda)\n","        return obj\n","\n","    def __array_finalize__(self, obj):\n","        if obj is None: return\n","        self.backward       = getattr(obj, 'backward',      lambda x: None)\n","        self.to             = getattr(obj, 'to',            lambda x: obj)\n","        self.grad           = getattr(obj, 'grad',          None)\n","        self.requires_grad  = getattr(obj, 'requires_grad', None)\n","\n","    class no_grad():\n","\n","        '''\n","        Synergizes with Tensor: By entering the tensor with no_grad scope,\n","        the Tensor.requires_grad singleton will swap to False.\n","        '''\n","\n","        def __enter__(self):\n","            # When tape scope is entered, stop asking tensors to record gradients\n","            Tensor.requires_grad = False\n","            return self\n","\n","        def __exit__(self, exc_type, exc_val, exc_tb):\n","            # When tape scope is exited, let Diffable start recording to self.operation\n","            Tensor.requires_grad = True"],"metadata":{"id":"oqBwVfbvLUeh","executionInfo":{"status":"ok","timestamp":1728578153421,"user_tz":300,"elapsed":419,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from abc import ABC, abstractmethod  # # For abstract method support\n","\n","class Diffable(ABC):\n","    \"\"\"\n","        We use these to represent differentiable layers which we can compute gradients for.\n","    \"\"\"\n","\n","    def to(self, device):\n","        return self         # Just there to ignore device setting calls\n","\n","    def __call__(self, *args, **kwargs):\n","\n","        ## The call method keeps track of method inputs and outputs\n","        self.argnames   = self.forward.__code__.co_varnames[1:]\n","        named_args      = {self.argnames[i] : args[i] for i in range(len(args))}\n","        self.input_dict = {**named_args, **kwargs}\n","        self.inputs     = [self.input_dict[arg] for arg in self.argnames if arg in self.input_dict.keys()]\n","        self.outputs    = self.forward(*args, **kwargs)\n","\n","        ## Make sure outputs are tensors and tie back to this layer\n","        list_outs = isinstance(self.outputs, list) or isinstance(self.outputs, tuple)\n","        if not list_outs:\n","            self.outputs = [self.outputs]\n","        self.outputs = [Tensor(out) for out in self.outputs]\n","        for out in self.outputs:\n","            out.backward = self.backward\n","\n","        # print(self.__class__.__name__.ljust(24), [v.shape for v in self.inputs], '->', [v.shape for v in self.outputs])\n","\n","        ## And then finally, it returns the output, thereby wrapping the forward\n","        return self.outputs if list_outs else self.outputs[0]\n","\n","    def parameters(self):\n","        \"\"\"Returns a list of parameters\"\"\"\n","        return ()\n","\n","    @abstractmethod\n","    def forward(self, x):\n","        \"\"\"Pass inputs through function. Can store inputs and outputs as instance variables\"\"\"\n","        pass\n","\n","    @abstractmethod\n","    def input_gradients(self):\n","        \"\"\"Returns local gradient of layer output w.r.t. input\"\"\"\n","        pass\n","\n","    def weight_gradients(self):\n","        \"\"\"Returns local gradient of layer output w.r.t. weights\"\"\"\n","        return []\n","\n","    @abstractmethod\n","    def backward(self, grad=np.array([[1]])):\n","        \"\"\"\n","        Propagate upstream gradient backwards by composing with local gradient\n","\n","        SCAFFOLD:\n","\n","        Differentiate with respect to layer parameters:\n","            For every param-gradient pair\n","            - If all Tensors or this tensor do not require gradients, then skip\n","            - Otherwise, compose upstream and local gradient\n","\n","        Differentiate with respect to layer input:\n","            For every input-gradient pair\n","            - If all Tensors or this tensor do not require gradients, then skip\n","            - Otherwise, compose upstream and local gradient\n","\n","        Usefulseful print boilerplate...:\n","            # print(f'Diffing w.r.t. \"{k}\": local = {g.shape} and upstream = {grad.shape}')\n","        \"\"\"\n","        pass"],"metadata":{"id":"0pL9iG1BLb6J","executionInfo":{"status":"ok","timestamp":1728578159046,"user_tz":300,"elapsed":487,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["**TODO : Loss**"],"metadata":{"id":"od1EJH53Knwl"}},{"cell_type":"code","source":["class MSELoss(Diffable):\n","    \"\"\"\n","    Calculates mean squared error loss and gradient w.r.t. inputs.\n","    Subclasses Diffable.\n","    \"\"\"\n","\n","    def forward(self, y_pred, y_true):\n","        \"\"\"Mean squared error forward pass!\"\"\"\n","        # Compute the MSE given predicted and actual labels\n","        self.y_pred = y_pred\n","        self.y_true = y_true\n","        # MSE formula: (1/n) * sum((y_pred - y_true)^2)\n","        mse = np.mean((y_pred - y_true) ** 2)\n","        return mse\n","\n","    def input_gradients(self):\n","        \"\"\"Mean squared error backpropagation!\"\"\"\n","        # Compute the gradient of MSE w.r.t y_pred and y_true\n","        n = len(self.y_pred)\n","        grad_y_pred = (2 / n) * (self.y_pred - self.y_true)\n","        grad_y_true = np.zeros_like(self.y_true)  # Gradients w.r.t. y_true are zero\n","        return grad_y_pred, grad_y_true\n","\n","    def backward(self, grad=np.array([[1]])):\n","        \"\"\"Mean squared error backpropagation!\"\"\"\n","        # Differentiate with respect to layer inputs\n","        grad_y_pred, grad_y_true = self.input_gradients()\n","\n","        # Compose the upstream gradient with this input's gradient\n","        grad_composed_pred = grad * grad_y_pred\n","\n","        # Set the gradient of y_pred tensor if it requires gradient\n","        if self.y_pred.requires_grad:\n","            self.y_pred.grad = grad_composed_pred\n","\n","        # y_true doesn't require gradients, but we handle it for completeness\n","        if self.y_true.requires_grad:\n","            self.y_true.grad = grad_y_true\n","\n","        # Pass the composed gradient backward (no further backward in this case)\n","        return grad_composed_pred\n"],"metadata":{"id":"btIWcK-dLpbd","executionInfo":{"status":"ok","timestamp":1728578162257,"user_tz":300,"elapsed":160,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class con:\n","    ## Control set using default PyTorch\n","    ytrue = torch.Tensor(Y0)\n","    ypred = torch_model(torch.Tensor(X0))\n","    loss_fn = nn.MSELoss()\n","\n","class exp:\n","    ## Experimental set using your own implementation\n","    ytrue = Tensor(Y0)\n","    ypred = Tensor(con.ypred.detach().numpy())\n","    loss_fn = MSELoss()\n","\n","def ypred_to_loss(ns):\n","    ## Compute loss using the control and experimental namespaces\n","    ns.loss = ns.loss_fn(ns.ypred, ns.ytrue)\n","    return ns.loss\n","\n","## Sanity Check 1: Make sure that the forward pass is the same (i.e. your implementation matches the control)\n","print(ypred_to_loss(con))\n","print(ypred_to_loss(exp))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"maf9vVLgMh4Z","executionInfo":{"status":"ok","timestamp":1728578350588,"user_tz":300,"elapsed":121,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"38f82ff0-2df5-4b73-8ce0-a71376d9d7b7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(3839.5854, grad_fn=<MseLossBackward0>)\n","3839.5852480675444\n"]}]},{"cell_type":"code","source":["## Sanity Check 2: Make sure that the backwards pass is the same\n","\n","con.ypred = con.ypred.detach()\n","con.ypred.requires_grad = True\n","#print(\"Before running backwards:\\n\", con.ypred.grad)\n","ypred_to_loss(con)\n","con.loss.backward()\n","#print(\"After running backwards:\\n\", con.ypred.grad)\n","\n","exp.ypred.grad = None\n","#print(\"Before running backwards:\\n\", np.round(exp.ypred.grad, 4))\n","ypred_to_loss(exp)\n","exp.loss.backward()\n","#print(\"After running backwards:\\n\", np.round(exp.ypred.grad, 4))\n","\n","max_diff = np.max(exp.ypred.grad - con.ypred.grad.detach().numpy())\n","print(f\"Maximum difference {max_diff} should be less than 0.00001\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pHgiBCG4Msa2","executionInfo":{"status":"ok","timestamp":1728578352818,"user_tz":300,"elapsed":179,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"ca8edfd6-fe07-481b-cb52-a54f7b1ce465"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum difference 3.6640815603838917e-08 should be less than 0.00001\n"]}]},{"cell_type":"markdown","source":["**LINEAR LAYER**"],"metadata":{"id":"1Uf05kQqN61X"}},{"cell_type":"code","source":["import numpy as np\n","\n","class Linear(Diffable):\n","    \"\"\"\n","    Standard linear/dense layer.\n","    Subclasses Diffable.\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, device=None, dtype=None):\n","        self.w, self.b = self.__class__._initialize_weight(in_features, out_features)\n","        self.inputs = None\n","        self.grad_w = None  # Gradient of weights\n","        self.grad_b = None  # Gradient of biases\n","\n","    def parameters(self):\n","        return [self.w, self.b]\n","\n","    def forward(self, inputs):\n","        \"\"\"\n","        Forward pass for a dense layer: Y = XW + B\n","        Inputs: (batch_size, in_features)\n","        Weights: (in_features, out_features)\n","        Bias: (1, out_features)\n","        \"\"\"\n","        self.inputs = inputs  # Store input for backprop\n","\n","        # Perform matrix multiplication: inputs @ weights + bias\n","        output = np.dot(inputs, self.w) + self.b\n","        return output\n","\n","    def weight_gradients(self):\n","        dw = np.dot(self.inputs.T, self.grad_w)  # Gradient w.r.t weights\n","        db = np.sum(self.grad_w, axis=0, keepdims=True)  # Gradient w.r.t biases\n","        return dw, db\n","\n","    def input_gradients(self):\n","        return np.dot(self.grad_w, self.w.T)\n","\n","    def backward(self, grad=np.array([[1]])):\n","        print(\"Entering backward function with grad:\", grad)\n","        self.grad_w = grad  # Store upstream gradient for weight gradient calculation\n","        dw, db = self.weight_gradients() # Compute gradients for weights and biases\n","        print(\"Computed weight gradient (dw):\", dw)\n","        print(\"Computed bias gradient (db):\", db)\n","        # Store the gradients (without updating the parameters directly)\n","        self.w.grad = dw  # Store gradient for weights\n","        self.b.grad = db  # Store gradient for biases\n","        print(\"Assigned weight gradient to self.w.grad:\", self.w.grad)\n","        print(\"Assigned bias gradient to self.b.grad:\", self.b.grad)\n","        # Return the gradient to propagate backwards to earlier layers\n","        return self.input_gradients()\n","\n","    @staticmethod\n","    def _initialize_weight(input_size, output_size):\n","        # Correct weight shape to (input_size, output_size)\n","        w = np.random.normal(0, 1, (input_size, output_size))  # (in_features, out_features)\n","        b = np.zeros((1, output_size))  # Bias is (1, out_features)\n","        return w, b\n","\n"],"metadata":{"id":"f6bX3O4oN-WQ","executionInfo":{"status":"ok","timestamp":1728578357498,"user_tz":300,"elapsed":120,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","# Example usage\n","class con:\n","    X0 = torch.tensor(X0, dtype=torch.float32, requires_grad=True)  # Set requires_grad correctly\n","    Y0 = torch.tensor(Y0, dtype=torch.float32, requires_grad=True)  # Set requires_grad correctly\n","    dense = nn.Linear(10, 1)  # Control model\n","    loss_fn = nn.MSELoss()\n","\n","class exp:\n","    X0 = torch.tensor(X0, dtype=torch.float32, requires_grad=True)  # Set requires_grad correctly\n","    Y0 = torch.tensor(Y0, dtype=torch.float32, requires_grad=True)  # Set requires_grad correctly\n","    dense = nn.Linear(10, 1)  # Experimental model\n","    dense.weight = nn.Parameter(con.dense.weight.detach().clone())  # Use Parameter for weights\n","    dense.bias = nn.Parameter(con.dense.bias.detach().clone())      # Use Parameter for bias\n","    loss_fn = nn.MSELoss()\n","\n","def x_to_loss(ns):\n","    ns.ypred = ns.dense(ns.X0)\n","    ns.loss = ns.loss_fn(ns.ypred, ns.Y0)\n","    #print(\"Loss inside x_to_loss:\", ns.loss)\n","    #print(\"Loss requires_grad:\", ns.loss.requires_grad)\n","    return ns.loss\n","\n","# Call x_to_loss and print outputs\n","loss_con = x_to_loss(con)\n","loss_exp = x_to_loss(exp)\n","\n","# Sanity checks\n","print(f\"Maximum difference: {np.max(con.ypred.detach().numpy() - exp.ypred.detach().numpy())} should be less than 0.00001\\n\")\n","print(f\"Losses: Control {loss_con.item()} vs Experimental {loss_exp.item()}\")\n","\n","# Print parameter values for debugging\n","print('\\nControl Params:', *list(con.dense.parameters()), sep='\\n')\n","print('\\nExperimental Params:', *list(exp.dense.parameters()), sep='\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ENmidL-kTrxL","executionInfo":{"status":"ok","timestamp":1728578480130,"user_tz":300,"elapsed":193,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"8fa9b4ea-369e-4b7f-947c-bd366b19360d"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Maximum difference: 0.0 should be less than 0.00001\n","\n","Losses: Control 29699.373046875 vs Experimental 29699.373046875\n","\n","Control Params:\n","Parameter containing:\n","tensor([[ 0.0421, -0.1437, -0.2759,  0.2651, -0.2627,  0.0977, -0.2938,  0.1480,\n","          0.0639,  0.2266]], requires_grad=True)\n","Parameter containing:\n","tensor([0.0327], requires_grad=True)\n","\n","Experimental Params:\n","Parameter containing:\n","tensor([[ 0.0421, -0.1437, -0.2759,  0.2651, -0.2627,  0.0977, -0.2938,  0.1480,\n","          0.0639,  0.2266]], requires_grad=True)\n","Parameter containing:\n","tensor([0.0327], requires_grad=True)\n"]}]},{"cell_type":"code","source":["## Sanity Check 2: Make sure that the backwards pass is the same\n","\n","con.X0 = con.X0.detach()\n","con.Y0 = con.Y0.detach()\n","for p in con.dense.parameters():\n","    if p.grad is None: continue\n","    p.grad.detach_()\n","    p.grad = None\n","\n","x_to_loss(con).backward()\n","print(\"After running backwards on weights:\")\n","print([p.grad for p in con.dense.parameters()])\n","\n","for p in exp.dense.parameters(): p.grad = None\n","x_to_loss(exp).backward()\n","\n","print(\"\\n\" + \"*\" * 100 + \"\\n\")\n","print(\"After running backwards on weights:\")\n","print([p.grad for p in exp.dense.parameters()])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AovaJSFyOf_0","executionInfo":{"status":"ok","timestamp":1728578483844,"user_tz":300,"elapsed":135,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"821ef96f-7c13-4369-f9ac-f4ffa81c993a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["After running backwards on weights:\n","[tensor([[-1.8602, -0.1088, -4.9841, -3.7174, -1.3137, -0.9007,  3.1145, -3.2770,\n","         -4.4843, -3.5200]]), tensor([-307.4061])]\n","\n","****************************************************************************************************\n","\n","After running backwards on weights:\n","[tensor([[-1.8602, -0.1088, -4.9841, -3.7174, -1.3137, -0.9007,  3.1145, -3.2770,\n","         -4.4843, -3.5200]]), tensor([-307.4061])]\n"]}]},{"cell_type":"markdown","source":["**STOCHASTIC GRADIENT DESCENT**"],"metadata":{"id":"HecpBbW3VyJh"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# SGD Optimizer Implementation\n","class SGD:\n","    \"\"\"\n","    Performs stochastic gradient descent with the specified learning rate.\n","    \"\"\"\n","    def __init__(self, params, lr, *args, **kwargs):\n","        self.params = params\n","        self.lr = lr\n","\n","    def zero_grad(self):\n","        \"\"\"\n","        Reset the gradients for all parameters.\n","        \"\"\"\n","        for param in self.params:\n","            if param.grad is not None:\n","                param.grad.zero_()\n","\n","    def step(self):\n","        \"\"\"\n","        Update parameters by subtracting the gradient multiplied by the learning rate.\n","        \"\"\"\n","        for param in self.params:\n","            if param.grad is not None:\n","                param.data -= self.lr * param.grad\n","\n","# FakeTorchModule provided earlier\n","class FakeTorchModule:\n","    \"\"\"\n","        Needed so that we can do manual linear regression.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.device = \"\"\n","\n","    def __call__(self, *args, **kwargs):\n","        return self.forward(*args, **kwargs)\n","\n","    def to(self, device):\n","        return self\n","\n","    def parameters(self):\n","        params = []\n","        for k, v in self.__dict__.items():\n","            params += getattr(v, 'parameters', lambda: [])()\n","        return params\n","\n","    def train(self):\n","        for p in self.parameters():\n","            p.requires_grad = getattr(p, 'required_grad', p.requires_grad)\n","\n","    def eval(self):\n","        for p in self.parameters():\n","            p.required_grad = p.requires_grad\n","            p.requires_grad = False\n","\n","class ManualRegression(FakeTorchModule):\n","    \"\"\"\n","    Allows us to use our custom Linear layer and SGD optimizer.\n","    Subclasses FakeTorchModule\n","    \"\"\"\n","\n","    def __init__(self, input_dims, output_dims):\n","        super().__init__()\n","        # Initialize the weights and bias for the linear layer manually\n","        self.w = torch.randn(input_dims, output_dims, requires_grad=True)\n","        self.b = torch.randn(output_dims, requires_grad=True)\n","        self.set_learning_rate()\n","\n","    def set_learning_rate(self, learning_rate=0.001):\n","        # Use custom SGD\n","        self.optimizer = SGD([self.w, self.b], lr=learning_rate)\n","\n","    def forward(self, x):\n","        # Linear layer computation: y = x @ w + b\n","        return x @ self.w + self.b\n","\n","    def step(self, X, Y, loss_fn):\n","        # Forward pass\n","        y_pred = self.forward(X)\n","\n","        # Calculate loss\n","        loss = loss_fn(y_pred, Y)\n","\n","        # Zero gradients\n","        self.optimizer.zero_grad()\n","\n","        # Backward pass\n","        loss.backward()\n","\n","        # Step optimizer\n","        self.optimizer.step()\n","\n","        return loss.item()\n","\n","class TrainTest2:\n","    def __init__(self):\n","        pass\n","\n","    def train_test(self, train_data, test_data, epochs=100):\n","        train_losses = []\n","        test_losses = []\n","        for epoch in range(epochs):\n","            X_train, Y_train = train_data[0]\n","            loss = self.step(X_train, Y_train, torch.nn.MSELoss())\n","            train_losses.append(loss)\n","\n","            if epoch % 50 == 0:\n","                print(f\"Epoch {epoch} / {epochs} - Train Loss: {loss}\")\n","\n","        return train_losses\n","\n","class ManualLinearRegression(ManualRegression, TrainTest2):\n","    def __init__(self, input_dims, output_dims):\n","        super().__init__(input_dims, output_dims)\n","        ## Now the model is initialized with custom SGD and linear layer\n","\n","# Initialize Manual Linear Regression Model\n","model = ManualLinearRegression(10, 1)\n","model.set_learning_rate(0.2)\n","\n","X0 = torch.tensor(X0, dtype=torch.float32)\n","Y0 = torch.tensor(Y0, dtype=torch.float32)\n","X1 = torch.tensor(X1, dtype=torch.float32)\n","Y1 = torch.tensor(Y1, dtype=torch.float32)\n","\n","# Train the model\n","manual_model_stats = model.train_test([[X0, Y0]], [[X1, Y1]], epochs=200)\n"],"metadata":{"id":"O_A36yyOPP0y","executionInfo":{"status":"ok","timestamp":1728578486590,"user_tz":300,"elapsed":310,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"883bb07d-b5b2-4da0-b321-9e68e7dd56e4"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0 / 200 - Train Loss: 29209.75\n","Epoch 50 / 200 - Train Loss: 5394.9453125\n","Epoch 100 / 200 - Train Loss: 4887.755859375\n","Epoch 150 / 200 - Train Loss: 4510.90087890625\n"]}]},{"cell_type":"markdown","source":["**Compare this model's performance to the first linear regression model**"],"metadata":{"id":"pRZvPs7BJXxh"}},{"cell_type":"code","source":["# Extract final training and testing losses\n","torch_final_train_loss = torch_model_stats[-1]['loss']\n","\n","manual_final_train_loss = manual_model_stats[-1]\n","\n","# Print comparison\n","print(f\"\"\"\n","Comparison of PyTorch and Manual FakeTorch Model (SGD):\n","-----------------------------------------------------\n","PyTorch Model Final Training Loss: {torch_final_train_loss}\n","Manual FakeTorch Model Final Training Loss: {manual_final_train_loss}\n","\n","\"\"\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U7GVFMlELWao","executionInfo":{"status":"ok","timestamp":1728578807213,"user_tz":300,"elapsed":136,"user":{"displayName":"Nathan Crosby","userId":"03222515215146913005"}},"outputId":"8d3258c0-8093-457e-e351-4cde4af12e25"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Comparison of PyTorch and Manual FakeTorch Model (SGD):\n","-----------------------------------------------------\n","PyTorch Model Final Training Loss: 3844.04296875\n","Manual FakeTorch Model Final Training Loss: 4232.11962890625\n","\n","\n"]}]}]}